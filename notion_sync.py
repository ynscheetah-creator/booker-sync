Elbette, konuÅŸtuÄŸumuz tÃ¼m geliÅŸtirmeleri, dÃ¼zeltmeleri ve en son istediÄŸiniz deÄŸiÅŸiklikleri (belirli sÃ¼tunlarÄ±n hariÃ§ tutulmasÄ±) iÃ§eren `notion_sync.py` dosyasÄ±nÄ±n son ve tam halini aÅŸaÄŸÄ±da bulabilirsiniz.

```python
# notion_sync.py
import os
from typing import Dict, Any, Optional
from notion_client import Client
from utils import (
    get_env, as_title, as_rich, as_url, as_number, as_multi_select
)
from google_books_api import fetch_from_google_books
from openlibrary_api import fetch_from_openlibrary
from goodreads_scraper import fetch_goodreads
import re
from datetime import datetime, timezone, timedelta
import logging

# --- CONSTANTS ---
NOTION_TOKEN = get_env("NOTION_TOKEN")
DATABASE_ID = get_env("NOTION_DATABASE_ID")
RECENT_EDIT_HOURS = int(get_env("RECENT_EDIT_HOURS", "24"))

# --- INITIALIZATION ---
if not NOTION_TOKEN or not DATABASE_ID:
    raise RuntimeError("âŒ NOTION_TOKEN ve NOTION_DATABASE_ID ortam deÄŸiÅŸkenleri ayarlanmalÄ±!")
notion = Client(auth=NOTION_TOKEN)

# --- HELPER FUNCTIONS ---
def _get_prop_value(p: Dict[str, Any]) -> Optional[str]:
    """Mevcut sayfadaki property'yi string olarak Ã§ek."""
    if p is None: return None
    t = p.get("type")
    try:
        if t == "title":
            arr = p.get("title", [])
            return "".join([x.get("plain_text", "") for x in arr]) if arr else None
        if t == "rich_text":
            arr = p.get("rich_text", [])
            return "".join([x.get("plain_text", "") for x in arr]) if arr else None
        if t == "url":
            return p.get("url")
        if t == "number":
            return str(p.get("number")) if p.get("number") is not None else None
        if t == "multi_select":
            arr = p.get("multi_select", [])
            return ", ".join([x.get("name", "") for x in arr]) if arr else None
    except (KeyError, IndexError):
        return None
    return None

def _was_recently_edited(page: Dict[str, Any]) -> bool:
    """Sayfa son X saat iÃ§inde dÃ¼zenlendi mi?"""
    try:
        last_edited = page.get("last_edited_time")
        if not last_edited: return False
        edited_time = datetime.fromisoformat(last_edited.replace("Z", "+00:00"))
        return (datetime.now(timezone.utc) - edited_time) < timedelta(hours=RECENT_EDIT_HOURS)
    except Exception:
        return False

def _needs_update(props: Dict[str, Any]) -> bool:
    """Bu sayfa gÃ¼ncellenmeye ihtiyaÃ§ duyuyor mu?"""
    # EÄŸer bu temel alanlardan herhangi biri boÅŸsa gÃ¼ncelleme gerekir.
    required_fields = ["Author", "Cover URL", "Number of Pages", "Year Published", "Publisher"]
    return any(not _get_prop_value(props.get(field)) for field in required_fields)

def _merge_book_data(*sources: Dict[str, Optional[str]]) -> Dict[str, Optional[str]]:
    """Veri kaynaklarÄ±nÄ± Ã¶ncelik sÄ±rasÄ±na gÃ¶re akÄ±llÄ±ca birleÅŸtirir."""
    merged = {}
    for source in sources:
        if not source: continue
        for key, value in source.items():
            if key not in merged and value:
                merged[key] = value
    return merged

# --- CORE DATA FETCHING ---
def fetch_book_data_pipeline(
    title: Optional[str], author: Optional[str], isbn: Optional[str], goodreads_url: Optional[str]
) -> Dict[str, Optional[str]]:
    """Veri Ã§ekme akÄ±ÅŸÄ±nÄ± yÃ¶netir: Goodreads -> API'ler."""
    goodreads_data, api_data = {}, {}

    # 1. Ã–ncelik: Goodreads (en doÄŸru kaynak)
    if goodreads_url:
        try:
            goodreads_data = fetch_goodreads(goodreads_url)
        except Exception as e:
            logging.warning(f"  âš ï¸ Goodreads scraper hatasÄ±: {e}")

    # 2. ZenginleÅŸtirme/Yedek: API'ler
    search_title = goodreads_data.get("Title") or title
    search_author = goodreads_data.get("Author") or author
    search_isbn = goodreads_data.get("ISBN13") or goodreads_data.get("ISBN") or isbn

    try:
        if search_isbn:
            api_data = fetch_from_google_books(isbn=search_isbn) or \
                       fetch_from_openlibrary(isbn=search_isbn)
        elif search_title:
            api_data = fetch_from_google_books(title=search_title, author=search_author) or \
                       fetch_from_openlibrary(title=search_title, author=search_author)
    except Exception as e:
        logging.warning(f"  âš ï¸ API arama hatasÄ±: {e}")

    # 3. BirleÅŸtirme (Goodreads Ã¶ncelikli)
    final_data = _merge_book_data(goodreads_data, api_data)
    if not final_data:
        logging.warning("  âš ï¸ HiÃ§bir kaynaktan veri bulunamadÄ±.")
    return final_data

# --- NOTION UPDATE LOGIC ---
def _build_updates(
    scraped: Dict[str, Optional[str]], existing_props: Dict[str, Any], force: bool
) -> Dict[str, Any]:
    """Kod tekrarÄ± olmadan Notion gÃ¼ncelleme gÃ¶vdesini oluÅŸturur."""
    updates = {}
    # Sadece sizin kullandÄ±ÄŸÄ±nÄ±z alanlarÄ± iÃ§eren property map
    prop_map = {
        "Title": ("Title", as_title),
        "Author": ("Author", as_multi_select),
        "Translator": ("Translator", as_multi_select),
        "goodreadsURL": ("goodreadsURL", as_url),
        "Cover URL": ("Cover URL", as_url),
        "Publisher": ("Publisher", as_rich),
        "Year Published": ("Year Published", as_number),
        "Original Publication Year": ("Original Publication Year", as_number),
        "Number of Pages": ("Number of Pages", as_number),
        "Description": ("Description", as_rich),
        "Language": ("Language", as_rich),
    }

    for prop_name, (scraped_key, formatter) in prop_map.items():
        existing_val = _get_prop_value(existing_props.get(prop_name))
        scraped_val = scraped.get(scraped_key)
        if scraped_val and (force or not existing_val):
            formatted_value = formatter(scraped_val)
            if formatted_value:
                updates[prop_name] = formatted_value
    
    # ISBN iÃ§in Ã¶zel kontrol
    existing_isbn = _get_prop_value(existing_props.get("ISBN"))
    isbn_val = scraped.get("ISBN13") or scraped.get("ISBN")
    if isbn_val and (force or not existing_isbn):
        updates["ISBN"] = as_rich(isbn_val)
        
    return updates

def _update_page_cover(page_id: str, cover_url: Optional[str]):
    if not cover_url: return
    try:
        notion.pages.update(page_id=page_id, cover={"type": "external", "external": {"url": cover_url}})
        logging.info("  ğŸ“¸ Kapak fotoÄŸrafÄ± gÃ¼ncellendi.")
    except Exception as e:
        logging.warning(f"  âš ï¸ Kapak gÃ¼ncellenemedi: {e}")

# --- MAIN RUNNER ---
def run_once():
    """Notion veritabanÄ±nÄ± tarar ve eksik bilgileri tamamlar."""
    logging.info("ğŸš€ AkÄ±llÄ± Senkronizasyon BaÅŸlatÄ±lÄ±yor...")
    logging.info(f"ğŸ”„ Son {RECENT_EDIT_HOURS} saatte dÃ¼zenlenenler tamamen gÃ¼ncellenecek.")
    
    all_pages = []
    start_cursor = None
    while True:
        try:
            response = notion.databases.query(database_id=DATABASE_ID, start_cursor=start_cursor, page_size=100)
            all_pages.extend(response["results"])
            if not response["has_more"]: break
            start_cursor = response["next_cursor"]
        except Exception as e:
            logging.error(f"âŒ Notion veritabanÄ± okunurken hata oluÅŸtu: {e}")
            return # ProgramÄ± durdur

    logging.info(f"ğŸ“š Notion'da {len(all_pages)} sayfa bulundu.\n")

    for idx, page in enumerate(all_pages, 1):
        props = page.get("properties", {})
        page_id = page["id"]
        
        force_update = _was_recently_edited(page)
        needs_update = _needs_update(props)

        if not force_update and not needs_update:
            continue

        title = _get_prop_value(props.get("Title"))
        gr_url = _get_prop_value(props.get("goodreadsURL"))
        display_name = title or gr_url or page_id
        
        logging.info(f"--- [{idx}/{len(all_pages)}] ğŸ“–: {display_name[:70]} ---")
        if force_update:
            logging.info("  ğŸ”„ YakÄ±n zamanda dÃ¼zenlendi, tam gÃ¼ncelleme yapÄ±lacak.")
        else:
            logging.info("  â„¹ï¸ Eksik alanlar var, doldurulacak.")

        scraped_data = fetch_book_data_pipeline(
            title=title,
            author=_get_prop_value(props.get("Author")),
            isbn=_get_prop_value(props.get("ISBN")),
            goodreads_url=gr_url,
        )

        if not scraped_data:
            logging.warning("  -> Veri bulunamadÄ±, atlanÄ±yor.\n")
            continue

        # Scraper baÅŸarÄ±sÄ±z olur ve baÅŸlÄ±k bulamazsa, iÅŸlem yapmayÄ± engelle
        if not scraped_data.get("Title") and not title:
            logging.warning("  -> BaÅŸlÄ±k bulunamadÄ±ÄŸÄ± iÃ§in bu sayfa atlanÄ±yor.\n")
            continue

        updates = _build_updates(scraped_data, props, force=force_update)

        if not updates:
            logging.info("  -> Eklenecek yeni bilgi yok.\n")
            continue
            
        # EÄŸer bir gÃ¼ncelleme yapÄ±lacaksa ve bu gÃ¼ncellemede 'Title' yoksa,
        # Notion'un hata vermemesi iÃ§in mevcut 'Title' bilgisini ekle.
        if "Title" not in updates:
            existing_title_prop = props.get("Title")
            if existing_title_prop:
                updates["Title"] = existing_title_prop

        try:
            notion.pages.update(page_id=page_id, properties=updates)
            logging.info(f"  âœ… Notion gÃ¼ncellendi: {', '.join(updates.keys())}")

            # Kapak fotoÄŸrafÄ±nÄ± da gÃ¼ncelle
            existing_cover = page.get("cover")
            if scraped_data.get("Cover URL") and (force_update or not existing_cover):
                _update_page_cover(page_id, scraped_data.get("Cover URL"))
            
            print() # Estetik iÃ§in bir boÅŸluk
        except Exception as e:
            logging.error(f"  âŒ Notion gÃ¼ncelleme hatasÄ±: {e}\n")
    
    logging.info("=" * 50)
    logging.info("âœ… AkÄ±llÄ± Senkronizasyon TamamlandÄ±!")
    logging.info("=" * 50)
```
